{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# https://github.com/wiso/StatisticsTutorialATLASItalia17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other resources\n",
    "\n",
    "* PhD school lectures https://github.com/wiso/StatisticsLectures\n",
    "* [Kyle Cranmer](http://orcid.org/0000-0002-5769-7094) [lectures](https://indico.cern.ch/event/117033/other-view?view=standard) and [proceedings](https://cds.cern.ch/record/2004587/files/arXiv:1503.07622.pdf) at 2011 ESHEP (see page 3 for many books)\n",
    "* Kyle Cranmer [lectures](https://indico.cern.ch/event/243641/) for summer students in 2013\n",
    "* Glen Cowan [Statistical Data Analysis for Particle Physics](http://www.pp.rhul.ac.uk/~cowan/stat_aachen.html) and [other](http://www.pp.rhul.ac.uk/~cowan/) lectures\n",
    "* [Luca Lista](http://people.na.infn.it/~lista/Statistics/) with RooStats examples\n",
    "* [Asymptotic formulae for likelihood-based tests of new physics](https://arxiv.org/pdf/1007.1727v3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<strong>Statistic</strong>: a function of the data (the mean, the number of observed events, ...)\n",
    "\n",
    "<strong>p-value</strong>: the probability to obtaining a result equal to or \"more extreme\" than what was actually observed\n",
    "What \"more extreme\" means? It depends on question we want to answer\n",
    "\n",
    "<strong>Likelihood</strong>: $\\mathcal{L}(\\theta) = P(\\text{data}|\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing\n",
    "\n",
    "A statistical hypothesis is a hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables\n",
    "\n",
    "A statistical hypothesis test is a method of statistical inference\n",
    "\n",
    "The goal of the hypothesis testing is to determine if the null ($H_0$) hypothesis can be\n",
    "rejected. A statistical test can either reject (prove false) or fail to reject (fail to\n",
    "prove false) a null hypothesis, but never prove it true (i.e., failing to reject a null\n",
    "hypothesis does not prove it true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normal significance\n",
    "\n",
    "Usually p-values are translated in a more-friendly normal-significance (number of sigmas) $z$. With the one-tail definition this is the value (quantile) corresponding to a certain p-value for a standard gaussian:\n",
    "\n",
    "$$ \\int_z^\\infty N[x| 0, 1] dx = \\text{p-value}$$\n",
    "\n",
    "or $ \\int_z^\\infty + \\int_{-\\infty}^{-z} N[x|0,1] dx = \\text{p-value}$ for the two-tail definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Taking into account the definition of the cumulative density function of a normal distribution, $\\Phi(z) = \\int_{-\\infty}^{z} N[x| 0, 1] dx$, this can be written as:\n",
    "\n",
    "$$ z = \\Phi^{-1}(1 - \\text{p-value})$$\n",
    "\n",
    "or taking into account the definition of the survival function $SF(z) = 1 - CDF(z)$:\n",
    "\n",
    "$$ z = \\text{SF}^{-1}(\\text{p-value})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error type\n",
    "\n",
    "   * Type I error: false positive (excessive credulity). Rate = $\\alpha = P(\\text{reject } H_0|H_0 \\text{ is true})$\n",
    "   * Type II error: false negative (excessive skepticism). Rate = $\\beta = P(\\text{don't reject } H_0|H_0 \\text{ is false})$\n",
    "   \n",
    "   * Power of the test: $1-\\beta$\n",
    "   * Remember that \"true\"/\"false\" refers to $H_1$, the alternative hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis testing in practice\n",
    "\n",
    "   * Define the null hypothesis you want to try to reject\n",
    "   * Define the observables (number of events in the signal region, ...)\n",
    "   * Fix the rate of type I error $\\alpha$ of the test statistics (5%, $5\\sigma$, ...)\n",
    "   * Define the test statistic (trying to maximize the power $1-\\beta$)\n",
    "   * Find the rejection region in the observable space, which is the region where $H_0$ is rejected (p-value $<\\alpha$)\n",
    "   * Do the experiment\n",
    "   * If the outcome is outside the acceptance region reject the null-hypothesis\n",
    "   \n",
    "In complex example one don't compute the acceptance region, but just compute the observed p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neymanâ€“Pearson lemma\n",
    "Having two simple hypotheses (no additional parameters) $H_0: \\theta = \\theta_0$ and $H_1: \\theta = \\theta_1$, the likelihood-ratio test:\n",
    "$$\n",
    "\\Lambda(x) = \\frac{L(\\theta_0|x)}{L(\\theta_1|x)}\n",
    "$$\n",
    "\n",
    "which rejects $H_0$ in favour of $H_1$ when $\\Lambda \\leq k_\\alpha$ (rejection region) with $\\alpha=P(\\Lambda(X)\\leq k_\\alpha|H_0)$ is the most powerful test with size $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Profile likelihood ratio\n",
    "\n",
    "Suppose you have composite hypothesis as \n",
    "\n",
    "$$H_0: \\theta\\in\\Theta_0 \\quad H_1: \\theta\\in\\Theta_0^C$$\n",
    "\n",
    "where $\\Theta_0 = \\{s=0, \\ldots\\}$, $\\theta\\in\\Theta_0^C = \\{s\\neq 0, \\ldots\\}$, \n",
    "\n",
    "$$\\lambda(x) = \\frac{\\sup_{\\theta\\in\\Theta_0}{L(\\theta|x)}}{\\sup_{\\theta\\in\\Theta}{L(\\theta|x)}}$$\n",
    "\n",
    "with $\\Theta_0 \\subset \\Theta$. For example it can be $H_0: s=0$, $H_1: s\\neq 0$\n",
    "\n",
    "$$\\lambda(x) = \\frac{L(s=0, \\hat{\\hat\\theta}(0)|x)}{L(\\hat{s}, \\hat\\theta|x)}$$\n",
    "\n",
    "where $\\hat{\\hat\\theta}(0)$ is the value of $\\theta$ which optimize the likelihood for $s=0$ (conditioned likelihood), while $\\hat{s}$ and $\\hat{\\theta}$ are the values that optimize the likelihood without any constrains (unconditioned likelihood).\n",
    "\n",
    "<small>It varies between 0 and 1, low values mean that the observed result is less likely to occur under the null hypothesis as compared to the alternative.\n",
    "\n",
    "The profile likelihood ratio is nearly an optimal test-statistics</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<small>As shown it is important to have an analytically expression $f_q$ of the distribution of the test-statistics $q$ to compute the p-value: $\\text{p-value} = \\int_{q^\\text{obs}}^{\\infty} f_q(q) dq$. Otherwise toys must be run.</small>\n",
    "\n",
    "## Wilks's theorem\n",
    "\n",
    "The quantity $t=-2\\log(\\lambda)$ is aymptotically (large sample) distributed as a $\\chi^2$ distribution with $n=\\text{dim}(\\Theta)-\\text{dim}(\\Theta_0)$ degrees of freedom when $H_0$ is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test statistics $t_\\mu$ and $\\tilde{t}_\\mu$\n",
    "\n",
    "$$ t_\\mu = -2\\log\\lambda (\\mu) = -2\\log \\frac{L(\\mu, \\hat{\\hat{\\theta}}(\\mu))}{L(\\hat{\\mu}, \\hat{\\theta})}$$\n",
    "\n",
    "High value means incompatiblity with data. If we want to test a specific $\\mu$ we can compute the p-value $= \\int_{t_{\\mu, obs}}^\\infty f(t_\\mu|\\mu) dt_\\mu$. Values can be excluded because they are too low, or too high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Usually we can assume that $\\mu\\geq 0$, so we need a new test statistic:\n",
    "\n",
    "$$ \\tilde{t}_\\mu = -2\\log \\tilde\\lambda(\\mu)$$\n",
    "\n",
    "$$ \\tilde\\lambda(\\mu) = \\begin{cases} \n",
    "      \\hfill \\frac{L(0, \\hat{\\hat{\\theta}}(\\mu=0))}{L(\\hat{\\mu}, \\hat{\\theta})}    \\hfill & \\hat{\\mu} < 0 \\\\\n",
    "      \\hfill \\frac{L(\\mu, \\hat{\\hat{\\theta}}(\\mu))}{L(\\hat{\\mu}, \\hat{\\theta})} \\hfill & \\hat{\\mu} \\geq 0 \\\\\n",
    "  \\end{cases}$$\n",
    "  \n",
    "Also in this case, values can be excluded because they are too low, or too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $q_0$ statistics for discovery of positive signal\n",
    "\n",
    "For discovery we want to exclude the hypothesis $s=0$ (background-only), assuming $\\mu\\geq 0$. Defining $q_0 = \\tilde{t}_0$:\n",
    "\n",
    "$$ q_0 = \n",
    "\\begin{cases} \n",
    "-2\\log\\lambda(0)\\qquad &\\hat \\mu\\geq 0 \\\\\n",
    "0\\qquad &\\hat \\mu < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If $\\hat \\mu<0$ it means that we are observing less events than the one predicted by the background-only model. Since we are truncating the the definition of test statistics we are not considering downward fluctuation as discrepancies with the model. High value of $\\hat\\mu$ means high value of $q_0$ and large discrepancy with the background-only model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $q_\\mu$ statistic for exclusion\n",
    "Suppose we want to put un upper limit, so we define as null hypothesis to exclude the hypotesis signal+background with $\\mu$ as signal multiplier.\n",
    "\n",
    "$$\n",
    "q_\\mu=\\begin{cases}\n",
    "-2\\log\\lambda(\\mu)\\qquad & \\hat\\mu \\leq \\mu\\\\\n",
    "0 \\qquad & \\hat\\mu > \\mu\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "we set $q_\\mu=0$ when observing a value of $\\mu$ greater than the one we are observing since we don't want it to enter in the rejection region when doing an upper limit; we don't want that upper fluctuation count as bad agreement with data."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
